{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Naive Bayes\n",
    "In this project, our goal is to build a spam filter using Naive Bayes that with an accuracy of at least 80% can classify messages as \"spam\" or \"non-spam\".\n",
    "\n",
    "To classify messages as spam or non-spam a computer:\n",
    "1. Learns how humans classify messages.\n",
    "2. Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "3. Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "Our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the __multinomial Naive Bayes__ algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the authors' papers.\n",
    "\n",
    "Let's read in the file using pandas. It is tab-seperated and it includes no headers, so we'll account for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spam_sms = pd.read_csv('SMSSpamCollection', sep = '\\t', header = None, names=['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ham` corresponds to non-spam and the meaning of `spam` is quite self-explanatorily the opposite. Let's determine what percentage of the collection is spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_sms['Label'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.4% of the messages in the collection are marked as spam, and the remaining 86.6% as non-spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test set\n",
    "Once our spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter, we're first going to split our dataset into two categories:\n",
    "- A training set, which we'll use to \"train\" the computer how to classify messages.\n",
    "- A test set, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,572 messages, which means that:\n",
    "- The training set will have 4,458 messages (about 80% of the dataset).\n",
    "- The test set will have 1,114 messages (about 20% of the dataset).\n",
    "\n",
    "For this project, our goal is to create a spam filter that classifies new messages with an accuracy greater than 80% — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam).\n",
    "\n",
    "We need this divide to be random, so we'll use the `DataFrame.sample()`-method for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized = spam_sms.sample(frac = 1, random_state = 1)\n",
    "\n",
    "cutoff_index = round(len(randomized) * 0.8)\n",
    "\n",
    "training = spam_sms[:cutoff_index].reset_index(drop=True)\n",
    "\n",
    "testing = spam_sms[cutoff_index:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the percentage of spam in the respective sets and see if they appear representative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     86.496187\n",
      "spam    13.503813\n",
      "Name: Label, dtype: float64 \n",
      "\n",
      "ham     86.983842\n",
      "spam    13.016158\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(training['Label'].value_counts(normalize=True) * 100, '\\n')\n",
    "print(testing['Label'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are both quite similar to our original percentages. This seems good enough for our purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "To make our classifications, we need to clean our data a bit so that it is in a more appropriate format.\n",
    "\n",
    "## Punctuation and letter case\n",
    "Firstly, we'll want to remove all punctuation from the messages and turn them all into lower case. The regular expression '\\W' detect any character that is not from a-z, A-Z or 0-9. We'll utilize this to replace such characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point  crazy   available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar    joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor    u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don t think he goes to usf  he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  go until jurong point  crazy   available only ...\n",
       "1   ham                      ok lar    joking wif u oni   \n",
       "2  spam  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3   ham  u dun say so early hor    u c already then say   \n",
       "4   ham  nah i don t think he goes to usf  he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['SMS'] = training['SMS'].str.replace('\\W', ' ')\n",
    "training['SMS'] = training['SMS'].str.lower()\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vocabulary\n",
    "Next, we will compile all words used in a vocabulary (a set of all unique words used):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['SMS'] = training['SMS'].str.split()\n",
    "\n",
    "word_set = set()\n",
    "for message in training['SMS']:\n",
    "    for word in message:\n",
    "        word_set.add(word)\n",
    "\n",
    "vocab = list(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we have 7813 unique words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7813"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final training set\n",
    "Lastly, we will transform the `SMS`-column into several columns, each representing a different word in the vocabulary. The values will represent how many occurences of that word we have in the corresponding messages.\n",
    "\n",
    "To do this we first create a dictionary, `words_per_sms`, where each key corresponds to a word in the vocabulary and each value corresponds to a vector representing how many times that word was used in the respective rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_per_sms = {unique_word: [0] * len(training['SMS']) for unique_word in vocab}\n",
    "\n",
    "for index, sms in enumerate(training['SMS']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use this dictionary when creating a new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pd.DataFrame(word_counts_per_sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we merge this dataframe with the `Label`-series, using the `pd.concat()`-function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>sink</th>\n",
       "      <th>yrs</th>\n",
       "      <th>lonely</th>\n",
       "      <th>glory</th>\n",
       "      <th>fps</th>\n",
       "      <th>hg</th>\n",
       "      <th>mi</th>\n",
       "      <th>aig</th>\n",
       "      <th>...</th>\n",
       "      <th>slowly</th>\n",
       "      <th>condition</th>\n",
       "      <th>nigeria</th>\n",
       "      <th>flute</th>\n",
       "      <th>07</th>\n",
       "      <th>4742</th>\n",
       "      <th>haventcn</th>\n",
       "      <th>09061702893</th>\n",
       "      <th>anna</th>\n",
       "      <th>chennai</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[nah, i, don, t, think, he, goes, to, usf, he,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  sink  yrs  lonely  \\\n",
       "0   ham  [go, until, jurong, point, crazy, available, o...     0    0       0   \n",
       "1   ham                     [ok, lar, joking, wif, u, oni]     0    0       0   \n",
       "2  spam  [free, entry, in, 2, a, wkly, comp, to, win, f...     0    0       0   \n",
       "3   ham  [u, dun, say, so, early, hor, u, c, already, t...     0    0       0   \n",
       "4   ham  [nah, i, don, t, think, he, goes, to, usf, he,...     0    0       0   \n",
       "\n",
       "   glory  fps  hg  mi  aig  ...  slowly  condition  nigeria  flute  07  4742  \\\n",
       "0      0    0   0   0    0  ...       0          0        0      0   0     0   \n",
       "1      0    0   0   0    0  ...       0          0        0      0   0     0   \n",
       "2      0    0   0   0    0  ...       0          0        0      0   0     0   \n",
       "3      0    0   0   0    0  ...       0          0        0      0   0     0   \n",
       "4      0    0   0   0    0  ...       0          0        0      0   0     0   \n",
       "\n",
       "   haventcn  09061702893  anna  chennai  \n",
       "0         0            0     0        0  \n",
       "1         0            0     0        0  \n",
       "2         0            0     0        0  \n",
       "3         0            0     0        0  \n",
       "4         0            0     0        0  \n",
       "\n",
       "[5 rows x 7815 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_final = pd.concat([training, word_counts], axis = 1)\n",
    "training_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculations\n",
    "The classification will be done comparing the following two equations:\n",
    "![alt text](classification.png \"Classification\")\n",
    "To calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we use the following equations (utilizing [additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing) to avoid certain issues with 0-count words):\n",
    "![alt text](calc.png \"Conditional probability\")\n",
    "In this case alpha will be set to 1, that is, we'll be using *Laplace smoothing*.\n",
    "\n",
    "Let's start doing the necessary calculations to make our classification possible.\n",
    "## Constants\n",
    "We will begin by calculating the constant values; P(Spam) and P(Ham), followed by NSpam, NHam and NVocabulary.\n",
    "Lastly we'll initialize a variable named alpha with a value of 1 (in accordance with the forementioned laplace smoothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam = (training_final['Label'] == 'spam').sum() / training_final.shape[0]\n",
    "\n",
    "p_ham = 1 - p_spam\n",
    "\n",
    "N_spam = training_final.loc[training_final['Label'] == 'spam', 'SMS'].apply(len).sum() #The number of words in all spam messages\n",
    "\n",
    "N_ham  = training_final.loc[training_final['Label'] == 'ham', 'SMS'].apply(len).sum() #The number of words in all non-spam messages\n",
    "\n",
    "N_vocab = len(vocab) #Total number of unique words\n",
    "\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "The probability values of P(wi|Spam) and P(wi|ham) will be different depending on the word. These area called parameters and it is what we'll be calculating next. A key point is that these will remain constant as long as our training set doesn't change.\n",
    "\n",
    "Recall that Nwi|Spam is equal to the number of times the word wi occurs in all the spam messages, while Nwi|Ham is equal to the number of times the word wi occurs in all the ham messages.\n",
    "\n",
    "The fact that all of these will be pre-calculated before any classification is done, contributes to making the algorithm very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing two dictionaries a key for every unique word, each with the initial value 0\n",
    "p_cond_spam = dict.fromkeys(vocab, 0)\n",
    "p_cond_ham = dict.fromkeys(vocab, 0)\n",
    "\n",
    "spam_messages = training_final[training_final['Label'] == 'spam']\n",
    "ham_messages = training_final[training_final['Label'] == 'ham']\n",
    "\n",
    "for word in vocab:\n",
    "    p_cond_spam[word] = (spam_messages[word].sum() + alpha) / (N_spam + N_vocab * alpha)\n",
    "    p_cond_ham[word] = (ham_messages[word].sum() + alpha) / (N_ham + N_vocab * alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying a new message\n",
    "With parameters and constants calculated, we can now start creating the spam filter. It can be understood according to below:\n",
    "- Takes in as input a new message (w1, w2, ..., wn)\n",
    "- Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "- Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "    - If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "    - If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "    - If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help.\n",
    "    \n",
    "For every incoming message we'll do a similar cleaning like before:\n",
    "- Remove punctuation\n",
    "- Transform to lower case\n",
    "- Split string to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    for word in message:\n",
    "        if word in p_cond_spam:\n",
    "            p_spam_given_message *= p_cond_spam[word]\n",
    "            \n",
    "        if word in p_cond_ham:\n",
    "            p_ham_given_message *= p_cond_ham[word]\n",
    "\n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that funtion done, let's try it out with two messages, one of which obviously is spam and one that is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 1.1946675407236375e-25\n",
      "P(Ham|message): 2.5383900524029554e-27\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "classify(\"WINNER!! This is the secret code to unlock the money: C3421.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 6.307218028475593e-25\n",
      "P(Ham|message): 4.417049039139304e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "classify(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function outputs the desired result for our two test examples. So far so good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the spam filter's accuracy\n",
    "Next, we want to look at the testing set we compiled in the beginning, to see with what accuracy we can succeed to classify the messages as spam and non-spam. First off, we'll alter our previous `classify`-function to return the classification instead of printing it. We can now use the function to create a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in p_cond_spam:\n",
    "            p_spam_given_message *= p_cond_spam[word]\n",
    "            \n",
    "        if word in p_cond_ham:\n",
    "            p_ham_given_message *= p_cond_ham[word]\n",
    "            \n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure accuracy we'll divide the number of correctly classified messages with the total number of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9856373429084381\n"
     ]
    }
   ],
   "source": [
    "testing['predicted'] = testing['SMS'].apply(classify_test_set)\n",
    "\n",
    "correct = 0\n",
    "total = testing.shape[0]\n",
    "\n",
    "for index, row in testing.iterrows():\n",
    "    if classify_test_set(row['SMS']) == row['Label']:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to achieve an accuracy of around 98.6%. This is way above our goal of 80% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
