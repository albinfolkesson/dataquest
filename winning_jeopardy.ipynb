{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winning Jeopardy\n",
    "Jeopardy is a popular TV show in the US where participants answer questions to win money. It's been running for a few decades, and is a major force in popular culture. In this project, we'll work with a dataset of Jeopardy questions to figure out some patterns in the questions that could help us win.\n",
    "\n",
    "The dataset is named jeopardy.csv, and contains 20000 rows from the beginning of a full dataset of Jeopardy questions, which you can download [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/).\n",
    "\n",
    "Here are explanations of each column:\n",
    "- `Show Number` -- the Jeopardy episode number of the show this question was in.\n",
    "- `Air Date` -- the date the episode aired.\n",
    "- `Round` -- the round of Jeopardy that the question was asked in. Jeopardy has several rounds as each episode progresses.\n",
    "- `Category` -- the category of the question.\n",
    "- `Value` -- the number of dollars answering the question correctly is worth.\n",
    "- `Question` -- the text of the question.\n",
    "- `Answer` -- the text of the answer.\n",
    "\n",
    "Let's begin reading in our data set using pandas and look at some rows as well as the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Show Number    Air Date      Round                         Category  Value  \\\n",
      "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
      "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
      "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
      "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
      "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
      "\n",
      "                                            Question      Answer  \n",
      "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
      "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
      "2  The city of Yuma in this state has a record av...     Arizona  \n",
      "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
      "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  \n",
      "\n",
      " Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
      "       ' Question', ' Answer'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "jeopardy = pd.read_csv('jeopardy.csv')\n",
    "print(jeopardy.head(5))\n",
    "print(\"\\n\", jeopardy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe a couple of things:\n",
    "- `Air Date` is parsable. Let's re-read our data set and parse the dates.\n",
    "- Several columns have a spaces in front. After we've re-read the data set we'll remove these spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', 'Air Date', 'Round', 'Category', 'Value', 'Question',\n",
       "       'Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy = pd.read_csv('jeopardy.csv', parse_dates = [' Air Date'])\n",
    "cols = []\n",
    "for i in jeopardy.columns:\n",
    "    cols.append(i.strip())\n",
    "    \n",
    "jeopardy.columns = cols\n",
    "jeopardy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing text\n",
    "Before we can start doing analysis on the Jeopardy questions, we need to normalize all of the text columns (the Question and Answer columns). We will write a function to achieve this by converting to lower case and removing all punctuation. We will assign the cleaned values to two new columns; `clean_question` and `clean_answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_str(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(\"[^A-Za-z0-9\\s]\", \"\", s)\n",
    "    s = re.sub(\"\\s+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy['clean_question'] = jeopardy['Question'].apply(normalize_str)\n",
    "jeopardy['clean_answer'] = jeopardy['Answer'].apply(normalize_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing values\n",
    "Now the text-columns are normalized but the numerical `value`-column remains to be normalized. We will write a corresponding function to remove any punctuation and convert it to a numerical data type. If the conversion has an error we'll just put in 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_val(i):\n",
    "    i = re.sub(\"[^A-Za-z0-9\\s]\", \"\", i)\n",
    "    try:\n",
    "        i = int(i)\n",
    "    except Exception:\n",
    "        i = 0\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy['clean_value'] = jeopardy['Value'].apply(normalize_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_answer</th>\n",
       "      <th>clean_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>for the last 8 years of his life galileo was u...</td>\n",
       "      <td>copernicus</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>no 2 1912 olympian football star at carlisle i...</td>\n",
       "      <td>jim thorpe</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>the city of yuma in this state has a record av...</td>\n",
       "      <td>arizona</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>in 1963 live on the art linkletter show this c...</td>\n",
       "      <td>mcdonalds</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>signer of the dec of indep framer of the const...</td>\n",
       "      <td>john adams</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>3-LETTER WORDS</td>\n",
       "      <td>$200</td>\n",
       "      <td>In the title of an Aesop fable, this insect sh...</td>\n",
       "      <td>the ant</td>\n",
       "      <td>in the title of an aesop fable this insect sha...</td>\n",
       "      <td>the ant</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$400</td>\n",
       "      <td>Built in 312 B.C. to link Rome &amp; the South of ...</td>\n",
       "      <td>the Appian Way</td>\n",
       "      <td>built in 312 bc to link rome the south of ital...</td>\n",
       "      <td>the appian way</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$400</td>\n",
       "      <td>No. 8: 30 steals for the Birmingham Barons; 2,...</td>\n",
       "      <td>Michael Jordan</td>\n",
       "      <td>no 8 30 steals for the birmingham barons 2306 ...</td>\n",
       "      <td>michael jordan</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$400</td>\n",
       "      <td>In the winter of 1971-72, a record 1,122 inche...</td>\n",
       "      <td>Washington</td>\n",
       "      <td>in the winter of 197172 a record 1122 inches o...</td>\n",
       "      <td>washington</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$400</td>\n",
       "      <td>This housewares store was named for the packag...</td>\n",
       "      <td>Crate &amp; Barrel</td>\n",
       "      <td>this housewares store was named for the packag...</td>\n",
       "      <td>crate barrel</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$400</td>\n",
       "      <td>\"And away we go\"</td>\n",
       "      <td>Jackie Gleason</td>\n",
       "      <td>and away we go</td>\n",
       "      <td>jackie gleason</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>3-LETTER WORDS</td>\n",
       "      <td>$400</td>\n",
       "      <td>Cows regurgitate this from the first stomach t...</td>\n",
       "      <td>the cud</td>\n",
       "      <td>cows regurgitate this from the first stomach t...</td>\n",
       "      <td>the cud</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$600</td>\n",
       "      <td>In 1000 Rajaraja I of the Cholas battled to ta...</td>\n",
       "      <td>Ceylon (or Sri Lanka)</td>\n",
       "      <td>in 1000 rajaraja i of the cholas battled to ta...</td>\n",
       "      <td>ceylon or sri lanka</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$600</td>\n",
       "      <td>No. 1: Lettered in hoops, football &amp; lacrosse ...</td>\n",
       "      <td>Jim Brown</td>\n",
       "      <td>no 1 lettered in hoops football lacrosse at sy...</td>\n",
       "      <td>jim brown</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$600</td>\n",
       "      <td>On June 28, 1994 the nat'l weather service beg...</td>\n",
       "      <td>the UV index</td>\n",
       "      <td>on june 28 1994 the natl weather service began...</td>\n",
       "      <td>the uv index</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Show Number   Air Date      Round                         Category Value  \\\n",
       "0          4680 2004-12-31  Jeopardy!                          HISTORY  $200   \n",
       "1          4680 2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES  $200   \n",
       "2          4680 2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...  $200   \n",
       "3          4680 2004-12-31  Jeopardy!                 THE COMPANY LINE  $200   \n",
       "4          4680 2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES  $200   \n",
       "5          4680 2004-12-31  Jeopardy!                   3-LETTER WORDS  $200   \n",
       "6          4680 2004-12-31  Jeopardy!                          HISTORY  $400   \n",
       "7          4680 2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES  $400   \n",
       "8          4680 2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...  $400   \n",
       "9          4680 2004-12-31  Jeopardy!                 THE COMPANY LINE  $400   \n",
       "10         4680 2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES  $400   \n",
       "11         4680 2004-12-31  Jeopardy!                   3-LETTER WORDS  $400   \n",
       "12         4680 2004-12-31  Jeopardy!                          HISTORY  $600   \n",
       "13         4680 2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES  $600   \n",
       "14         4680 2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...  $600   \n",
       "\n",
       "                                             Question                 Answer  \\\n",
       "0   For the last 8 years of his life, Galileo was ...             Copernicus   \n",
       "1   No. 2: 1912 Olympian; football star at Carlisl...             Jim Thorpe   \n",
       "2   The city of Yuma in this state has a record av...                Arizona   \n",
       "3   In 1963, live on \"The Art Linkletter Show\", th...             McDonald's   \n",
       "4   Signer of the Dec. of Indep., framer of the Co...             John Adams   \n",
       "5   In the title of an Aesop fable, this insect sh...                the ant   \n",
       "6   Built in 312 B.C. to link Rome & the South of ...         the Appian Way   \n",
       "7   No. 8: 30 steals for the Birmingham Barons; 2,...         Michael Jordan   \n",
       "8   In the winter of 1971-72, a record 1,122 inche...             Washington   \n",
       "9   This housewares store was named for the packag...         Crate & Barrel   \n",
       "10                                   \"And away we go\"         Jackie Gleason   \n",
       "11  Cows regurgitate this from the first stomach t...                the cud   \n",
       "12  In 1000 Rajaraja I of the Cholas battled to ta...  Ceylon (or Sri Lanka)   \n",
       "13  No. 1: Lettered in hoops, football & lacrosse ...              Jim Brown   \n",
       "14  On June 28, 1994 the nat'l weather service beg...           the UV index   \n",
       "\n",
       "                                       clean_question         clean_answer  \\\n",
       "0   for the last 8 years of his life galileo was u...           copernicus   \n",
       "1   no 2 1912 olympian football star at carlisle i...           jim thorpe   \n",
       "2   the city of yuma in this state has a record av...              arizona   \n",
       "3   in 1963 live on the art linkletter show this c...            mcdonalds   \n",
       "4   signer of the dec of indep framer of the const...           john adams   \n",
       "5   in the title of an aesop fable this insect sha...              the ant   \n",
       "6   built in 312 bc to link rome the south of ital...       the appian way   \n",
       "7   no 8 30 steals for the birmingham barons 2306 ...       michael jordan   \n",
       "8   in the winter of 197172 a record 1122 inches o...           washington   \n",
       "9   this housewares store was named for the packag...         crate barrel   \n",
       "10                                     and away we go       jackie gleason   \n",
       "11  cows regurgitate this from the first stomach t...              the cud   \n",
       "12  in 1000 rajaraja i of the cholas battled to ta...  ceylon or sri lanka   \n",
       "13  no 1 lettered in hoops football lacrosse at sy...            jim brown   \n",
       "14  on june 28 1994 the natl weather service began...         the uv index   \n",
       "\n",
       "    clean_value  \n",
       "0           200  \n",
       "1           200  \n",
       "2           200  \n",
       "3           200  \n",
       "4           200  \n",
       "5           200  \n",
       "6           400  \n",
       "7           400  \n",
       "8           400  \n",
       "9           400  \n",
       "10          400  \n",
       "11          400  \n",
       "12          600  \n",
       "13          600  \n",
       "14          600  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers in questions\n",
    "In order to figure out whether to study past questions, study general knowledge, or not study it all, it would be helpful to figure out two things:\n",
    "\n",
    "- How often the answer is deducible from the question.\n",
    "- How often new questions are repeats of older questions.\n",
    "\n",
    "We can answer the second question by seeing how often complex words (> 6 characters) reoccur. We can answer the first question by seeing how many times words in the answer also occur in the question. We'll work on the first question now, and come back to the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matches(row):\n",
    "    split_answer = row['clean_answer'].split()\n",
    "    split_question = row['clean_question'].split()\n",
    "    \n",
    "    match_count = 0\n",
    "    \n",
    "    # 'the' commonly occurs in both questions and answer and is not meaningful in this context so we'll remove it\n",
    "    if 'the' in split_answer:\n",
    "        split_answer.remove('the')\n",
    "    \n",
    "    # Returning 0 if length of split_answer is 0, to avoid risking dividing with 0 later\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    \n",
    "    question_words = set(split_question)\n",
    "    \n",
    "    for word in split_answer:\n",
    "        if word in question_words:\n",
    "            match_count += 1\n",
    "    \n",
    "    return match_count/len(split_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.059001965249777744"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_in_question = jeopardy.apply(count_matches, axis = 1)\n",
    "answer_in_question.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears only 5.9% of the words in the answers also appears in the questions. This is not a huge number and it likely means we can't really rely on deducting an answer purely from hearing the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recycled questions\n",
    "Another interesting matter to study is how often questions are recycled. If a large share of questions are repetitions of old ones, studying previous questions can be a viable strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6894031359073217"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap = []\n",
    "terms_used = set()\n",
    "jeopardy.sort_values(by='Air Date', inplace = True)\n",
    "\n",
    "for index, row in jeopardy.iterrows():\n",
    "    split_question = row['clean_question'].split()\n",
    "    split_question = [question for question in split_question if len(question) > 5]\n",
    "    \n",
    "    match_count = 0\n",
    "    \n",
    "    for word in split_question:\n",
    "        if word in terms_used:\n",
    "            match_count += 1\n",
    "        else:\n",
    "            terms_used.add(word)\n",
    "            \n",
    "    if len(split_question) > 0:\n",
    "        match_count /= len(split_question)\n",
    "    \n",
    "    question_overlap.append(match_count)\n",
    "\n",
    "jeopardy['question_overlap'] = question_overlap\n",
    "jeopardy['question_overlap'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 68.9% of questions appear to have been recycled which is quite a large share. Keep in mind though that the data we're looking at only is made up of 10% of the entire data set. Thus, we don't look at all phrases and it may not be fully representative. However, the results are interesting and legitimize looking further at recycled questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low value vs High value questions\n",
    "Let's say we only want to study questions that pertain to high value questions instead of low value questions. This will help us earn more money when you're on Jeopardy.\n",
    "\n",
    "You can actually figure out which terms correspond to high-value questions using a chi-squared test. You'll first need to narrow down the questions into two categories:\n",
    "\n",
    "- Low value -- Any row where Value is less than 800.\n",
    "- High value -- Any row where Value is greater than 800.\n",
    "\n",
    "You'll then be able to loop through each of the terms from the last screen, terms_used, and:\n",
    "- Find the number of low value questions the word occurs in.\n",
    "- Find the number of high value questions the word occurs in.\n",
    "- Find the percentage of questions the word occurs in.\n",
    "- Based on the percentage of questions the word occurs in, find expected counts.\n",
    "- Compute the chi squared value based on the expected counts and the observed counts for high and low value questions.\n",
    "\n",
    "You can then find the words with the biggest differences in usage between high and low value questions, by selecting the words with the highest associated chi-squared values. Doing this for all of the words would take a very long time, so we'll just do it for a small sample now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_val(row):\n",
    "    val = 0\n",
    "    if row[\"clean_value\"] > 800:\n",
    "        val = 1\n",
    "    return val\n",
    "\n",
    "jeopardy[\"high_value\"] = jeopardy.apply(set_val, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_low_high(word):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    for i, row in jeopardy.iterrows():\n",
    "        if word in row[\"clean_question\"].split(\" \"):\n",
    "            if row[\"high_value\"] == 0:\n",
    "                low_count += 1\n",
    "            else:\n",
    "                high_count += 1\n",
    "    return low_count, high_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0),\n",
       " (1, 2),\n",
       " (2, 0),\n",
       " (2, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (2, 1),\n",
       " (0, 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "comparison_terms = random.sample(terms_used, 10)\n",
    "comparison_terms\n",
    "\n",
    "observed_expected = []\n",
    "for word in comparison_terms:\n",
    "    observed_expected.append(count_low_high(word))\n",
    "\n",
    "observed_expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the chi-squared test\n",
    "Now that we've found the observed counts for a few terms, we can compute the expected counts and the chi-squared value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the scipy.stats.chisquare function to compute the chi-squared value and p-value given the expected and observed counts.\n",
    "Append the results to chi_squared.\n",
    "Look over the chi-squared values and the associated p-values. Are there any statistically significant results? Write up your thoughts in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_value_count = jeopardy['high_value'].sum()\n",
    "low_value_count = jeopardy['high_value'].shape[0] - jeopardy['high_value'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=2.1177104383031944, pvalue=0.14560406868263753),\n",
       " Power_divergenceResult(statistic=0.803925692253768, pvalue=0.3699222378079571),\n",
       " Power_divergenceResult(statistic=0.803925692253768, pvalue=0.3699222378079571),\n",
       " Power_divergenceResult(statistic=0.401962846126884, pvalue=0.5260772985705469),\n",
       " Power_divergenceResult(statistic=2.487792117195675, pvalue=0.11473257634454047),\n",
       " Power_divergenceResult(statistic=2.487792117195675, pvalue=0.11473257634454047),\n",
       " Power_divergenceResult(statistic=2.487792117195675, pvalue=0.11473257634454047),\n",
       " Power_divergenceResult(statistic=0.03188116723440362, pvalue=0.8582887163235293),\n",
       " Power_divergenceResult(statistic=2.487792117195675, pvalue=0.11473257634454047)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "import numpy as np\n",
    "\n",
    "chi_squared = []\n",
    "for i in observed_expected:\n",
    "    total = sum(i)\n",
    "    total_prop = total / jeopardy.shape[0]\n",
    "    expected_low = total_prop * low_value_count\n",
    "    expected_high = total_prop * high_value_count\n",
    "    \n",
    "    observed = np.array([i[0], i[1]])\n",
    "    expected = np.array([expected_low, expected_high])\n",
    "    chi_squared.append(chisquare(observed, expected))\n",
    "\n",
    "chi_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-squared results\n",
    "p-values are all exceeding 0.10, indicating that there are no statistically significant differences between high value and low value rows. Many of the words, however, have a very low frequency as indicated earlier, which makes the chi-squared test less valid. It would be better to run the test only for words with relatively high frequencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
